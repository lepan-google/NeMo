diff --git a/examples/nlp/language_modeling/megatron_gpt_pretraining.py b/examples/nlp/language_modeling/megatron_gpt_pretraining.py
index 422319a38..0932080cb 100644
--- a/examples/nlp/language_modeling/megatron_gpt_pretraining.py
+++ b/examples/nlp/language_modeling/megatron_gpt_pretraining.py
@@ -17,6 +17,7 @@ from pathlib import Path
 
 # To suppress BF16 compile related issue in the CI runs with turing/V100
 import torch._dynamo
+import time
 import torch.multiprocessing as mp
 from omegaconf.omegaconf import OmegaConf, open_dict
 
@@ -26,12 +27,23 @@ from nemo.collections.nlp.parts.nlp_overrides import NLPSaveRestoreConnector
 from nemo.core.config import hydra_runner
 from nemo.utils import logging
 from nemo.utils.exp_manager import exp_manager
+from pytorch_lightning.trainer.connectors.checkpoint_connector import _CheckpointConnector
 
 torch._dynamo.config.suppress_errors = True
 
 mp.set_start_method("spawn", force=True)
 
 
+class _CustomCheckpointConnector(_CheckpointConnector):
+    def _restore_modules_and_callbacks(self, checkpoint_path=None) -> None:
+        logging.info(
+            f'Checkpoint loading starts at {time.time()}.'
+        )
+        super()._restore_modules_and_callbacks(checkpoint_path)
+        logging.info(
+            f'Checkpoint loading ends at {time.time()}.'
+        )
+
 @hydra_runner(config_path="conf", config_name="megatron_gpt_config")
 def main(cfg) -> None:
     logging.info("\n\n************** Experiment configuration ***********")
@@ -60,6 +72,7 @@ def main(cfg) -> None:
     else:
         model = MegatronGPTModel(cfg.model, trainer)
 
+    trainer._checkpoint_connector = _CustomCheckpointConnector(trainer)
     trainer.fit(model)
 
 
diff --git a/nemo/utils/callbacks/nemo_model_checkpoint.py b/nemo/utils/callbacks/nemo_model_checkpoint.py
index 9893b0806..13c138861 100644
--- a/nemo/utils/callbacks/nemo_model_checkpoint.py
+++ b/nemo/utils/callbacks/nemo_model_checkpoint.py
@@ -15,6 +15,7 @@
 import os
 import re
 import shutil
+import time
 from copy import deepcopy
 from pathlib import Path
 from typing import Any, Dict, Iterable, List, Optional, Tuple, Union
@@ -467,6 +468,9 @@ class NeMoModelCheckpoint(ModelCheckpoint):
                 self.deferred_ckpts_to_remove.append([])
             else:
                 storage_options = None
+            logging.info(
+                f'Checkpoint async save for step {trainer.global_step} starts at {time.time()}.'
+            )
             trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)
             if self.async_save:
                 logging.info(f'Scheduled async checkpoint save for {filepath}')
@@ -496,6 +500,9 @@ class NeMoModelCheckpoint(ModelCheckpoint):
                 return
 
             logging.info(f'Async checkpoint save for step {global_step} ({filepath}) finalized successfully.')
+            logging.info(
+                f'Checkpoint async save for step {global_step} ends at {time.time()}. - logging'
+            )
 
             # Remove checkpoints marked for removal by `self._remove_checkpoint`
             # For each finalization there is exactly one entry in self.deferred_ckpts_to_remove
