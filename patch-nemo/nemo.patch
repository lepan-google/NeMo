diff --git a/examples/nlp/language_modeling/megatron_gpt_pretraining.py b/examples/nlp/language_modeling/megatron_gpt_pretraining.py
index 422319a38..2f278fdce 100644
--- a/examples/nlp/language_modeling/megatron_gpt_pretraining.py
+++ b/examples/nlp/language_modeling/megatron_gpt_pretraining.py
@@ -17,6 +17,7 @@ from pathlib import Path
 
 # To suppress BF16 compile related issue in the CI runs with turing/V100
 import torch._dynamo
+import time
 import torch.multiprocessing as mp
 from omegaconf.omegaconf import OmegaConf, open_dict
 
@@ -44,23 +45,41 @@ def main(cfg) -> None:
     if cfg.model.get("restore_from_path") is not None:
         # Option 1: Restore only the model weights from a .nemo file
         logging.info(f"Continual training: loading weights from {cfg.model.restore_from_path}")
+        logging.info(
+            f'Checkpoint load from path {cfg.model.restore_from_path} starts at {time.time()} - logging'
+        )
         model = MegatronGPTModel.restore_from(
             restore_path=cfg.model.restore_from_path,
             override_config_path=cfg.model,
             trainer=trainer,
             save_restore_connector=NLPSaveRestoreConnector(),
         )
+        logging.info(
+            f'Checkpoint load from path {cfg.model.restore_from_path} ends at {time.time()} - logging'
+        )
     elif cfg.model.get("restore_from_ckpt") is not None:
         # Option 2: Restore both model weights and optimizer states from a PTL checkpoint
         logging.info(f"Continual training: loading weights and optimizer states from {cfg.model.restore_from_ckpt}")
+        logging.info(
+            f'Checkpoint load from ckpt {cfg.model.restore_from_ckpt} starts at {time.time()} - logging'
+        )
         trainer.ckpt_path = Path(cfg.model.restore_from_ckpt)
         model = MegatronGPTModel(cfg.model, trainer)
+        logging.info(
+            f'Checkpoint load from ckpt {cfg.model.restore_from_ckpt} ends at {time.time()} - logging'
+        )
 
     # Start new pretraining or resume from a checkpoint if it exists
     else:
         model = MegatronGPTModel(cfg.model, trainer)
 
+    logging.info(
+        f'Training starts at {time.time()} - logging'
+    )
     trainer.fit(model)
+    logging.info(
+        f'Training ends at {time.time()} - logging'
+    )
 
 
 if __name__ == '__main__':
diff --git a/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py b/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py
index 755d67e12..d86e3b7b6 100644
--- a/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py
+++ b/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py
@@ -14,6 +14,7 @@
 
 import itertools
 import os
+import time
 import queue
 import warnings
 from contextlib import nullcontext
@@ -1639,7 +1640,13 @@ class MegatronGPTModel(MegatronBaseModel, TextGeneration):
 
         resume_checkpoint_path = self.trainer.ckpt_path
         if resume_checkpoint_path and not self.continue_training:
+            logging.info(
+                f'Extract consumed samples from ckpt {resume_checkpoint_path} starts at {time.time()} - logging'
+            )
             init_consumed_samples = self._extract_consumed_samples_from_ckpt(resume_checkpoint_path)
+            logging.info(
+                f'Extract consumed samples from ckpt {resume_checkpoint_path} ends at {time.time()} - logging'
+            )
         else:
             init_consumed_samples = 0
         self.init_consumed_samples = init_consumed_samples
diff --git a/nemo/core/connectors/save_restore_connector.py b/nemo/core/connectors/save_restore_connector.py
index 23b38510b..5cadc5087 100644
--- a/nemo/core/connectors/save_restore_connector.py
+++ b/nemo/core/connectors/save_restore_connector.py
@@ -22,6 +22,7 @@ import uuid
 from typing import Optional, Set, Union
 
 import torch
+import time
 from omegaconf import DictConfig, OmegaConf
 from omegaconf.omegaconf import open_dict
 from pytorch_lightning.trainer.trainer import Trainer
@@ -254,6 +255,9 @@ class SaveRestoreConnector:
         """
         # Get path where the command is executed - the artifacts will be "retrieved" there
         # (original .nemo behavior)
+        logging.info(
+            f'Connector restores from {restore_path} starts at {time.time()} - logging'
+        )
         loaded_params = self.load_config_and_state_dict(
             calling_cls,
             restore_path,
@@ -270,6 +274,9 @@ class SaveRestoreConnector:
         state_dict = self.modify_state_dict(conf, state_dict)
         self.load_instance_with_state_dict(instance, state_dict, strict)
         logging.info(f'Model {instance.__class__.__name__} was successfully restored from {restore_path}.')
+        logging.info(
+            f'Connector restores from {restore_path} ends at {time.time()} - logging'
+        )
         return instance
 
     def extract_state_dict_from(self, restore_path: str, save_dir: str, split_by_module: bool = False):
diff --git a/nemo/utils/callbacks/dist_ckpt_io.py b/nemo/utils/callbacks/dist_ckpt_io.py
index 091075488..b226f3b6f 100644
--- a/nemo/utils/callbacks/dist_ckpt_io.py
+++ b/nemo/utils/callbacks/dist_ckpt_io.py
@@ -126,8 +126,11 @@ class AsyncFinalizableCheckpointIO(_WrappingCheckpointIO):
         Applies underlying checkpoint_io finalize callback first, then the external one (postfix order).
         """
         external_finalize_fn = (storage_options or {}).pop('finalize_fn', None)
+        external_setup_fn = (storage_options or {}).pop('setup_fn', None)
         assert isinstance(self.checkpoint_io, AsyncCompatibleCheckpointIO), type(self.checkpoint_io)
         async_request = self.checkpoint_io.save_checkpoint(checkpoint, path, storage_options)
+        if external_setup_fn is not None:
+            async_request.setup_fn = external_setup_fn
         if external_finalize_fn is not None:
             async_request.add_finalize_fn(external_finalize_fn)
         call_idx = self.async_calls_queue.schedule_async_request(async_request)
diff --git a/nemo/utils/callbacks/nemo_model_checkpoint.py b/nemo/utils/callbacks/nemo_model_checkpoint.py
index 9893b0806..30e3cdd6e 100644
--- a/nemo/utils/callbacks/nemo_model_checkpoint.py
+++ b/nemo/utils/callbacks/nemo_model_checkpoint.py
@@ -15,6 +15,7 @@
 import os
 import re
 import shutil
+import time
 from copy import deepcopy
 from pathlib import Path
 from typing import Any, Dict, Iterable, List, Optional, Tuple, Union
@@ -457,22 +458,35 @@ class NeMoModelCheckpoint(ModelCheckpoint):
         else:
             # Async save passed the finalization function to checkpoint_io,
             # sync save calls the finalization function immediately after save.
+            setup_fn = self._get_setup_save_checkpoint_callback(
+                trainer.global_step)
             finalize_fn = self._get_finalize_save_checkpoint_callback(trainer, filepath, trainer.global_step)
             if self.async_save:
                 checkpoint_io = trainer.strategy.checkpoint_io
                 if not isinstance(checkpoint_io, AsyncFinalizableCheckpointIO):
                     raise ValueError('Async save requires async compatible CheckpointIO')
-                storage_options = dict(finalize_fn=finalize_fn)
+                storage_options = dict(
+                    finalize_fn=finalize_fn, setup_fn=setup_fn)
                 # Each upcoming ckpt removal request will be executed as part of this save finalization
                 self.deferred_ckpts_to_remove.append([])
             else:
                 storage_options = None
+            logging.info(
+                f'Checkpoint async save for step {trainer.global_step} starts at {time.time()}. - logging'
+            )
             trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)
             if self.async_save:
                 logging.info(f'Scheduled async checkpoint save for {filepath}')
             else:
                 finalize_fn()
 
+    def _get_setup_save_checkpoint_callback(self, global_step: int):
+        def _cb():
+            logging.info(
+                f'Checkpoint async save setup for step {global_step} starts at {time.time()}. - logging'
+            )
+        return _cb
+
     def _get_finalize_save_checkpoint_callback(
         self, trainer: 'pytorch_lightning.Trainer', filepath: str, global_step: int
     ):
@@ -496,6 +510,11 @@ class NeMoModelCheckpoint(ModelCheckpoint):
                 return
 
             logging.info(f'Async checkpoint save for step {global_step} ({filepath}) finalized successfully.')
+            logging.info(
+                f'Async checkpoint save for step {global_step} ({filepath}) finalized successfully at {time.time()}.')
+            logging.info(
+                f'Checkpoint async save for step {global_step} ends at {time.time()}. - logging'
+            )
 
             # Remove checkpoints marked for removal by `self._remove_checkpoint`
             # For each finalization there is exactly one entry in self.deferred_ckpts_to_remove
